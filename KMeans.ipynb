{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yasir Hassan\n",
    "# k-means algorithm & PCA \n",
    "# For Anticline images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import related libraries\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getData function definition\n",
    "def get_data(folder):\n",
    "    #Load the data and labels from the given folder\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for seismic_type in os.listdir(folder):\n",
    "             if not seismic_type.startswith('.'):\n",
    "                 if seismic_type in ['Class1']:\n",
    "                     label = '0'\n",
    "                 else:\n",
    "                     label = '1'\n",
    "                 for image_filename in os.listdir(folder + seismic_type):\n",
    "                     img_file = cv2.imread(folder + seismic_type + '/' + image_filename, 0)\n",
    "                     if img_file is not None:\n",
    "                         # Downsample the image to 120, 160, 3\n",
    "                         # img_file = scipy.misc.imresize(arr=img_file, size=(120, 160, 3))\n",
    "                         img_arr = np.asarray(img_file)\n",
    "                         X.append(img_arr)\n",
    "                         y.append(label)\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from the source\n",
    "X, y = get_data(\"C:/Users/Input File1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data randomnly\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert X data from 3d to 2d array\n",
    "nsamples, nx, ny = X_train.shape\n",
    "nsamples2, nx2, ny2 = X_test.shape\n",
    "X_train = X_train.reshape((nsamples,nx*ny))\n",
    "X_test = X_test.reshape((nsamples2,nx2*ny2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y train and y test data to fit the classifier\n",
    "nsamples  = y_train.shape\n",
    "nsamples2 = y_test.shape\n",
    "y_train = y_train.reshape(nsamples)\n",
    "y_test = y_test.reshape(nsamples2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformer class - StandardScaler and fit it \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stdScaler = StandardScaler()\n",
    "stdScaler.fit(X_train)\n",
    "X_train_scaled = stdScaler.transform(X_train)\n",
    "X_test_scaled = stdScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=0, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the KMeans for clustering and fit it with X train\n",
    "# using two clusters, from domain knowledge.\n",
    "kmeans = KMeans(n_clusters = 2, random_state=0)\n",
    "kmeans.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster memberships:\n",
      "[0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1\n",
      " 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0\n",
      " 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
      " 1 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0\n",
      " 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0\n",
      " 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0\n",
      " 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cluster memberships:\\n{}\".format(kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = kmeans.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y_test from string to int (number)\n",
    "y_test_num = [ int(item) for item in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test_num, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35 11]\n",
      " [ 0 44]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8777777777777778\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test_num, y_test_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying PCA\n",
    "# keep the first two principal components of the data\n",
    "pca = PCA(n_components = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit PCA model with X_train_scaled\n",
    "pca.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (360, 76800)\n",
      "Reduced shape: (360, 2)\n"
     ]
    }
   ],
   "source": [
    "# transform data onto the first two principal components\n",
    "X_pca = pca.transform(X_train_scaled)\n",
    "print(\"Original shape: {}\".format(str(X_train_scaled.shape)))\n",
    "print(\"Reduced shape: {}\".format(str(X_pca.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best Kmeans number of clusters:\n",
    "# using Elbow method to evaluate the best Kmeans n_clusters.\n",
    "# Elbow method gives an idea on what a good k number of clusters.\n",
    "# Pick k at the spot where SSE starts to flatten out and \n",
    "# forming an elbow.\n",
    "# SSE is the Sum of Squared Distance between data points \n",
    "# and their assigned clustersâ€™ centroids.\n",
    "\n",
    "sse = []\n",
    "list_k = list(range(1, 10))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(X_train_scaled)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the Elbow method above, n_clusters = 2 appeared to be the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
